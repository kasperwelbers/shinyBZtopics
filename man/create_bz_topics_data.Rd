% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/prepare_bz_data.r
\name{create_bz_topics_data}
\alias{create_bz_topics_data}
\title{Prepare data for topicbrowser}
\usage{
create_bz_topics_data(
  d,
  pos = c("NOUN", "PROPN"),
  min_docfreq = 5,
  max_docfreq_pct = 0.5,
  remove = NULL,
  deduplicate = NA,
  K = 50,
  ...
)
}
\arguments{
\item{d}{A data.frame with the columns "headline", "medium", "date" and "text". All other column will be included as metadata.}

\item{pos}{A selection of POS tags to use. See \url{https://universaldependencies.org/u/pos/} for the universal dependencies POS tags.}

\item{min_docfreq}{The minimum number of documents (absolute value) in which a feature needs to occur to be used in the topic model}

\item{max_docfreq_pct}{The maximum number of documents in which a feature can occur, given as a percentage of the total number of documents.}

\item{remove}{Specific features to be removed, given as a character vector}

\item{deduplicate}{Optionally, a similarity threshold for duplicates (only for articles in same medium within 24 hour diffence)}

\item{K}{The number of topics in the stm model}

\item{...}{arguments passed to \code{\link{stm}}}
}
\description{
Parses the texts and stores the results in a database, so that texts do not need to be parsed again if data is updated.
Then fits the STM model. The results are not returned, but the tcorpus and stm model are saved in the current working directory.
These files are then used by the \code{\link{run_topicbrowser}} function. Thus, this function only needs to be used the first time and
when new data is added.
}
